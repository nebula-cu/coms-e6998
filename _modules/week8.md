---
title: "Systems for Machine Learning: LLM Serving"
---
Oct 25
: [Efficient Memory Management for Large Language Models Serving with PagedAttention](https://dl.acm.org/doi/10.1145/3600006.3613165)
: [Taming Throughput-Latency Tradeoff in LLM Inference with Sarathi-Serve](https://www.usenix.org/conference/osdi24/presentation/agrawal)
: [DistServe: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving](https://www.usenix.org/conference/osdi24/presentation/zhong-yinmin)